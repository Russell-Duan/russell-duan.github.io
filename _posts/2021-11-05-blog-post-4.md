---
layout: post
title: Blog Post 4
---

# Blog Post: Spectral Clustering

## Introduction

In this blog post, we will be studying `spectral clustering` which is clustering algorithm used to identify different parts of data sets with complex structure. We first need to look at an example that doesn't need spectral clustering.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
![png](output_2_1.png)


*Clustering* is to divide this data sets into two groups, and we use k-means to achieve this task. K-means usually has good performance on circular blobs like below:


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![png](output_4_2.png)


### Harder Clustering

The data sets above are easy to classify. Now, let's take a look at some complex patterns.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x1a21f4f8d0>




![png](output_6_1.png)


We can make two meaningful clusters as well, but they are now crescents instead of blobs. We put the Euclidean coordinates of the data points into the matrix `X`, and put the labels of each point into the vector `y`. However, k-means worked poorly on this data because it is designed for circular clusters.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![png](output_8_2.png)


Clearly, the clusteing is incorrect.

Therefore, we will derive and implement spectral clustering which is able to correctly cluster the two crescents.

## Part A

At the beginning, we have to construct a similarity matrix $\mathbf{A}$ which has the shape `(n, n)`. We need a parameter called `epsilon` to construct $\mathbf{A}$ such that the entry `A[i,j]` is `1` is `X[i]` is within the distance `epsilon` of `X[j]`, and `0` otherwise.

However, we have to make the diagonal of $\mathbf{A}$ all zeros.

We use the build-in function `pairwise` of `sklearn.metrics` which takes an input $\mathbf{X}$ and returns a matrix, let's say $\mathbf{B}$. The entry `B[i, j]` is the distance between `X[i]` and `X[j]`. Therefore, we could easily compute $\mathbf{A}$ with the following codes.


```python
from sklearn.metrics import pairwise

epsilon = 0.4
A = pairwise_distances(X)
A = np.where(A <= epsilon, 1, 0) #assign 1 to those within the distance, 0 to others
np.fill_diagonal(A, 0) #fill the diagonal with zeros
A
```

    /Users/haonanduan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      dtype = np.float
    /Users/haonanduan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py:56: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      dtype = np.float





    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



## Part B

Now, we know which points are near from the matrix `A`, so that our task of clustering data points has become partitioning the rows and colunmns of `A`.

We need to define the *binary norm cut objective* of a matrix as follows:
$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$. 
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$ is the *degree* of row $i$ (the total number of all other rows related to row $i$ through $A$). The *volume* of cluster $C_0$ is a measure of the size of the cluster. 

Let's compute the cut term and the volume term separately.


#### B.1 The Cut Term

The cut term $\mathbf{cut}(C_0, C_1)$ indicates the number of nonzero entries in $\mathbf{A}$ that relate the points in two clusters. If there are so many points in C0 that are close to C1, the cut term will be large. However, such case doesn't usually happen since we expect the data points in two clusters are away from each other.

We are going to use two for loops to compute the cut term.


```python
def cut(A, y):
    sum = 0 #initialize the sum
    for i in range(np.shape(A)[0]): #loop over rows
        for j in range(np.shape(A)[1]): #loop over columnes
            if y[i] != y[j]: #the two points are not in the same cluster
                sum += A[i, j]
    
    return sum
```

Compute the cut objective for the true clusters `y`. Then, generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. Check the cut objective for the random labels. You should find that the cut objective for the true labels is *much* smaller than the cut objective for the random labels. 

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 


```python
fake_label = np.random.choice([0, 1], size=(200,))
cut(A, fake_label) #2232.0
cut(A, y) #26.0
```




    26.0



#### B.2 The Volume Term 

Let's take a look at the volumne term which simply measures the size of each cluster.

We are going to implement a function called `vols(A,y)` which outputs the size of two clusters as a tuple. By incorprating the cut term and the volume term within the `normcut(A, y)` function, we are able to compute the `binary norm cut`.


```python
def vols(A, y):
    vo1_0 = np.sum(A[y == 0,])
    vo1_1 = np.sum(A[y == 1,])
    return vo1_0, vo1_1

def normcut(A, y):
    vol_0, vol_1 = vols(A, y)
    return cut(A, y)*(1/vol_0 + 1/vol_1)
```

By comparing the norm cut of fake label and the actual label, the difference is large.


```python
normcut(A, fake_label) #2.0650268054241563
normcut(A, y) #0.02303682466323045
```




    0.02303682466323045



## Part C

Since we have known how to compute the `binary norm cut`, our next step is to minimize this term. Finding the best clustering. If we want to find the best clustering by brute-force algorithm, it will take forever, so that we need math to simplify this procss. We define $\mathbf{z}$ as follows:

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Now, the above equation can be used to calculate norm cut.

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

We will implement the function `transform(A, y)` to calculate the equation above.


```python
def transform(A, y):
    D = np.zeros(np.shape(A))
    np.fill_diagonal(D, np.sum(A,axis=1))
    z = np.zeros(len(y))
    vol_0, vol_1 = vols(A, y)
    z[y == 0] = 1/vol_0
    z[y == 1] = -1/vol_1
    return 2*(np.transpose(z).dot(D-A).dot(z))/(z@D@z)
```


```python
transform(A, y) #0.023036824663230267
np.isclose(normcut(A, y), transform(A, y))
```




    True



## Part D

Lastly, the final task is to minimize the following function.

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. 

By using the `minimize` function from `scipy.optimize`, we are able to minimize the function.


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

D = np.zeros(np.shape(A))

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize
z_min = minimize(orth_obj, z).x
```

## Part E

Plot the original data using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 


```python
plt.scatter(X[:,0], X[:,1], c = z_min < 0)
```
![png](output_27_1.png)


However, the clustering isn't correct. By setting the boundary to -0.0015, we could obtain a satisfying result.


```python
plt.scatter(X[:,0], X[:,1], c = z_min < -0.0015)
```
![png](output_29_1.png)


## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $\mathbf{z}$, subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. 

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $\mathbb{1}$ is actually the eigenvector with smallest eigenvalue of the matrix $\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$. 

> So, the vector $\mathbf{z}$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

Construct the matrix $\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $\mathbf{A}$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, plot the data again, using the sign of `z_eig` as the color. How did we do? 


```python
L = np.linalg.inv(D).dot(D-A) 
u, v = np.linalg.eig(L)
index = u.argsort()
z_eig = v[:, index[1]]
```


```python
plt.scatter(X[:,0], X[:,1], c = z_eig < 0)
```




    <matplotlib.collections.PathCollection at 0x1a210aee10>




![png](output_32_1.png)


In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

## Part G

We implement the function `spectral_clustering` to summarize all the steps above.


```python
def spectral_clustering(X, epsilon):
    """
    the spectral_clustering algorithm which outputs the label of data points
        
    Paramters
    ---------
    X : np.array, the matrix containing the Eucliean coordinates of each data point
    epsilon: float, the distance between the data points within the same cluster
    
    Returns
    ---------
    np.where(z_eig < 0, 1 ,0): np.array, the label of each data point
    """
    
    A = pairwise_distances(X)
    A = np.where(A <= epsilon, 1, 0)
    np.fill_diagonal(A, 0) #construct the similarity matrix
    
    D = np.zeros(np.shape(A))
    np.fill_diagonal(D, np.sum(A, axis=1))
    L = np.linalg.inv(D).dot(D-A) #construct the Laplacian matrix
    
    u, v = np.linalg.eig(L)
    index = u.argsort()
    z_eig = v[:, index[1]] #compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix.
    
    return np.where(z_eig < 0, 1 ,0) #return labels based on this eigenvector
```


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```
![png](output_36_2.png)


## Part H

Now we increase the noise to 0.1 and run our spectral clustering again.


```python
X, y = datasets.make_moons(n_samples=200, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```
![png](output_38_2.png)


Our clustering did well on noise 0.1. Now, we change our samples to 1000.


```python
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```
![png](output_40_2.png)


Even though some of the data points went off, the clustering still worked well.


```python
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```
![png](output_42_2.png)


## Part I

Now let's try our spectral clustering function on the bull's eye.


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![png](output_44_1.png)


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![png](output_46_2.png)


By setting epsilon to 0.3, our clustering successfully separates the two circles.


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.3))
```
![png](output_48_2.png)